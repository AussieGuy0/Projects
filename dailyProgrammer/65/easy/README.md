<div class="md"><p>Write a program that given a floating point number, gives the number of American dollar coins and bills needed to represent that number (rounded to the nearest 1/100, i.e. the nearest penny). For instance, if the float is 12.33, the result would be 1 ten-dollar bill, 2 one-dollar bills, 1 quarter, 1 nickel and 3 pennies.</p>
<p>For the purposes of this problem, these are the different denominations of the currency and their values:</p>
<ul>
<li>Penny: 1 cent</li>
<li>Nickel: 5 cent</li>
<li>Dime: 10 cent</li>
<li>Quarter: 25 cent</li>
<li>One-dollar bill</li>
<li>Five-dollar bill</li>
<li>Ten-dollar bill</li>
<li>Fifty-dollar bill</li>
<li>Hundred-dollar bill</li>
</ul>
<p>Sorry Thomas Jefferson, JFK and Sacagawea, but no two-dollar bills, half-dollars or dollar coins!</p>
<p>Your program can return the result in whatever format it wants, but I recommend just returning a list giving the number each coin or bill needed to make up the change. So, for instance, 12.33 could return [0,0,1,0,2,1,0,1,3] (here the denominations are ordered from most valuable, the hundred-dollar bill, to least valuable, the penny)</p>
<hr/>
<ul>
<li>Thanks to <a href="http://www.reddit.com/user/Medicalizawhat">Medicalizawhat</a> for submitting this problem in <a href="/r/dailyprogrammer_ideas">/r/dailyprogrammer_ideas</a>! And on behalf of the moderators, I'd like to thank everyone who submitted problems the last couple of days, it's been really helpful, and there are some great problems there! Keep it up, it really helps us out a lot!</li>
</ul>
</div>
